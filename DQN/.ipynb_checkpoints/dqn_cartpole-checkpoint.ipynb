{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from collections import namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "import math\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03719331, -0.01610895, -0.04308932, -0.02265111])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_id = 'CartPole-v0'\n",
    "\n",
    "env = gym.make(env_id)\n",
    "env.reset()\n",
    "# for _ in range(400):\n",
    "#     env.render()\n",
    "#     time.sleep(0.01)\n",
    "#     env.step(env.action_space.sample())   # take random actions\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking and using if CUDA is available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience Replay to have memory for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class exp_replay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return torch.cat(state), action, reward, torch.cat(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.005\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "\n",
    "replay_buffer_size = 20000\n",
    "learning_rate = 0.0001\n",
    "num_episodes = 200\n",
    "target_update_freq = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cart_loc(screen_width):\n",
    "    world_width = env.x_threshold*2\n",
    "    scale = screen_width/world_width\n",
    "#     print(scale)\n",
    "#     print(int(env.state[0]*scale + screen_width/2.0))\n",
    "    return int(env.state[0]*scale + screen_width/2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_screen():\n",
    "#         screen = env.render(mode='rgb_array')\n",
    "#         screen_1 = cv2.cvtColor(screen, cv2.COLOR_RGB2GRAY)\n",
    "#         r_screen = cv2.resize(screen_1, (84,84), interpolation=cv2.INTER_AREA)\n",
    "#         r_screen = np.array(r_screen)\n",
    "#         r_screen = np.expand_dims(r_screen,axis=0)\n",
    "#         r_screen = torch.Tensor(r_screen)\n",
    "#         return r_screen.unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "def get_screen():\n",
    "    screen = env.render(mode = 'rgb_array')\n",
    "    ### Capture screen return in shape (H,W,C) = (400,600,3)\n",
    "    \n",
    "    ### Cart is in lower half of screen. We remove the unuseful part of environment\n",
    "    screen_height, screen_width, _ = screen.shape\n",
    "    screen = screen[int(screen_height*0.4):int(screen_height*0.8),:]  # Screen Shape is now (160,600,3)\n",
    "#     print(screen.shape)\n",
    "\n",
    "    view_width = int(screen_width*0.6) # width required when pole becomes horizontal. \n",
    "#     print(view_width//2)\n",
    "    cart_loc = get_cart_loc(screen_width)\n",
    "\n",
    "    ### Extract screen where cart is locaed and discard the remaining part\n",
    "    if cart_loc < view_width//2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_loc > (screen_width - view_width//2):\n",
    "        slice_range = slice(-view_width)\n",
    "    else:\n",
    "        slice_range = slice(cart_loc - view_width//2, cart_loc + view_width//2)\n",
    "#     print(slice_range)\n",
    "    screen = screen[:,slice_range,:]\n",
    "#     print(screen.shape)\n",
    "    \n",
    "    \n",
    "    ### Pytorch work wwith format (C,H,W). So we take transpose\n",
    "    screen = screen.transpose((2,0,1))\n",
    "#     print(screen.shape)\n",
    "    \n",
    "    ### Convert ot float and normalize\n",
    "    screen = np.ascontiguousarray(screen, dtype = np.float32)/255\n",
    "    \n",
    "    ### Convert to tensor\n",
    "    screen = torch.from_numpy(screen)\n",
    "    \n",
    "    ### Adding dimension for batch\n",
    "    screen = resize(screen).unsqueeze(0).to(device)\n",
    "#     print(screen.shape)\n",
    "    return screen\n",
    "    \n",
    "\n",
    "### Image appears to be blur due to interpolation\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy())\n",
    "plt.title('Extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(dqn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "                return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        print(convh, convw)\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "init_screen = get_screen()\n",
    "init_screen = init_screen.squeeze(0)\n",
    "print(init_screen.shape[2])\n",
    "\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "policy_net = dqn(init_screen.shape[1], init_screen.shape[2], num_actions).to(device)\n",
    "target_net = dqn(init_screen.shape[1], init_screen.shape[2], num_actions).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr = learning_rate)\n",
    "\n",
    "memory = exp_replay(replay_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "def select_action(state):\n",
    "    global epsilon, epsilon_min\n",
    "    sample = random.random()\n",
    "    \n",
    "#     eps_threshold = epsilon_min + (epsilon-epsilon_min)*math.exp(-1.*steps_done/epsilon_decay)\n",
    "#     steps_done += 1\n",
    "#     print(eps_threshold)\n",
    "    if epsilon>epsilon_min:\n",
    "        epsilon = epsilon - epsilon_decay\n",
    "        \n",
    "    print(epsilon)\n",
    "    if sample < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward\n",
    "            \n",
    "    else:\n",
    "         with torch.no_grad():\n",
    "            q_val = policy_net(state)\n",
    "            action = torch.argmax(q_val).item()\n",
    "            \n",
    "    return action\n",
    "\n",
    "\n",
    "def plot_durations(scores,pause):\n",
    "    plt.ion()\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "\n",
    "    durations_t = torch.tensor(scores, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 20 episode averages and plot them too\n",
    "    if len(durations_t) >= 20:\n",
    "        means = durations_t.unfold(0, 20, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(19), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(pause)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    state_batch, action_batch, reward_batch, next_state_batch, done_batch = memory.sample(batch_size)\n",
    "    reward_batch = torch.tensor(reward_batch, device = device)\n",
    "    done_batch = torch.tensor(done_batch, device = device)\n",
    "#     print(done_batch)\n",
    "\n",
    "\n",
    "    q_val = policy_net(state_batch).to(device)\n",
    "    q_next = target_net(next_state_batch).to(device).detach()\n",
    "    q_target = policy_net(state_batch).to(device).detach()\n",
    "    target_net.eval()\n",
    "\n",
    "    batch_index = np.arange(batch_size)\n",
    "    action_values = torch.max(q_next,1)[0]\n",
    "\n",
    "    q_target[batch_index,action_batch] = reward_batch + gamma*action_values*done_batch\n",
    "    \n",
    "    loss = criterion(q_val, q_target).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_reward = []\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    done = False\n",
    "    total_reward  = 0\n",
    "    while not done:\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        next_state = current_screen - last_screen\n",
    "        \n",
    "        \n",
    "        memory.push(state, action, reward, next_state, done)\n",
    "        learn()     # Learning of model with help of experience replay\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "    ep_reward.append(total_reward)\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if ep % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_durations(ep_reward,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
